{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68cd99-a9e3-49bc-9dec-13807742875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from datacube.utils import geometry\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from feature_collection import feature_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a144d88-e16b-4c03-a1d6-d793e719c34f",
   "metadata": {},
   "source": [
    "## Read in cleaned data and select as required\n",
    "\n",
    "This step reads in the cleaned points, and determines which will be used for the training set. We make the following selections:\n",
    "* Only use points corresponding to a single crop (no multi-cropped fields)\n",
    "* Remove fallow fields\n",
    "* Only use classes with 10 or more observations (required for cross-validation during model training and evaluation)\n",
    "\n",
    "We use the geojson from the previous step (columns are the full, cleaned name, rather than the truncated, cleaned name used for shapefiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030dbe96-8cc9-46e5-a35f-2de2e84f27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to cleaned data from previous step\n",
    "path = \"../1_Prepare_samples_for_ML/results/points.geojson\"\n",
    "\n",
    "# Load input data\n",
    "input_data = gpd.read_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f493be0-9ac1-48ed-a5b3-e27621c8d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date fields to datetimes\n",
    "\n",
    "input_data[\"start\"] = pd.to_datetime(input_data[\"start\"], yearfirst=True)\n",
    "input_data[\"end\"] = pd.to_datetime(input_data[\"end\"], yearfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b1a2c-d322-4ca0-916f-2d53a23d0717",
   "metadata": {},
   "source": [
    "### Split into single crops and multiple crops\n",
    "\n",
    "Also remove fallow fields from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d3d06-984b-4f3e-b4d7-cf2a84a9f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with multiple crops or fallow fields\n",
    "multiple_crop_condition = input_data.loc[:, \"multiple_crops\"] == \"yes\"\n",
    "fallow_field_condition = input_data.loc[:, \"field_fallow\"] == \"yes\"\n",
    "\n",
    "# Split datasets\n",
    "single_crops = input_data.loc[\n",
    "    (multiple_crop_condition == False) & (fallow_field_condition == False), :\n",
    "].copy()\n",
    "\n",
    "multiple_crops = input_data.loc[\n",
    "    (multiple_crop_condition == True) & (fallow_field_condition == False), :\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed96555-7e8e-417a-83d3-42f7e78e98b0",
   "metadata": {},
   "source": [
    "## Explore and refine single crops dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b708a-f53b-4291-b31a-3574231fa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_crops.primary_crop.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f1b6e-48b5-4029-9035-55f80097bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with fewer than 10 observations, as it won't be possible to run cross-validation on these if only using a single pixel from each observation\n",
    "\n",
    "single_crops_subset = single_crops[single_crops.groupby('primary_crop').primary_crop.transform('count')>=10].reset_index(drop=True).copy()\n",
    "single_crops_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872259a-6ffa-4949-b3bc-c94b99cd6406",
   "metadata": {},
   "source": [
    "## Map crop types to numeric classes for prediction\n",
    "\n",
    "This step also saves out the mapping as a JSON file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956ac4e-94b0-4eaf-a704-af3527b353c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select field to label\n",
    "field = \"primary_crop\"\n",
    "\n",
    "# Fit label encoder to match classes to numeric labels\n",
    "le = LabelEncoder()\n",
    "le.fit(single_crops_subset[field])\n",
    "\n",
    "# Get a list of the crop types\n",
    "classes = list(le.classes_)\n",
    "\n",
    "# Assign numeric label for each class\n",
    "single_crops_subset[\"label\"] = le.transform(single_crops_subset[field])\n",
    "\n",
    "# Create a dictionary mapping classes to numeric labels\n",
    "class_dictionary = {crop_class: int(le.transform([crop_class])[0]) for crop_class in classes}\n",
    "print(\"Class Dictionary:\")\n",
    "print(class_dictionary)\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "\n",
    "# Export class dictionary\n",
    "with open(\"results/class_labels.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(class_dictionary, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b5922-9074-42ec-9de3-1faf41f0b80b",
   "metadata": {},
   "source": [
    "## Prepare geometry for feature extraction\n",
    "\n",
    "Either points or polygons can be used for extraction. If a point, the method will extract the pixel containing the point. If a polygon, the method will extract all pixels touching or within that polygon. \n",
    "\n",
    "The method uses 10m resolution pixels. As such, we recommend buffering the point to a square polygon of 30m across (15m on each side of the point). This should return nine pixels per point collected. Only use this if your points are more than 15 metres from the road or another field on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95882b81-291b-4e48-969d-6ae692e2d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a flag to convert to polygons:\n",
    "use_polygons = True\n",
    "\n",
    "if use_polygons:\n",
    "    # Convert from lat,lon to EPSG:6933 (projection in metres)\n",
    "    single_crops_subset = single_crops_subset.to_crs(\"EPSG:6933\")\n",
    "\n",
    "    # Buffer geometry to get a square - only if trying to sample multiple pixels\n",
    "    buffer_radius_m = 15\n",
    "    single_crops_subset.geometry = single_crops_subset.geometry.buffer(buffer_radius_m, cap_style=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c782d1-4a22-4b09-9c1b-ba9b9d988774",
   "metadata": {},
   "source": [
    "## Prepare query for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec206151-d6f1-4428-80f4-2bca47904a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = single_crops_subset.start.min()\n",
    "end_date = single_crops_subset.end.max()\n",
    "\n",
    "query_start_date = pd.Timestamp(\n",
    "    year=start_date.year, month=start_date.month, day=1\n",
    ") - pd.DateOffset(months=9)\n",
    "query_end_date = pd.Timestamp(\n",
    "    year=start_date.year, month=start_date.month, day=1\n",
    ") - pd.DateOffset(minutes=1)\n",
    "print(f\"Query start: {query_start_date}\")\n",
    "print(f\"Query end: {query_end_date}\")\n",
    "\n",
    "# Write a general query\n",
    "time = (query_start_date, query_end_date)\n",
    "resolution = (-10, 10)\n",
    "output_crs = \"EPSG:6933\"\n",
    "\n",
    "query = {\n",
    "    \"time\": time,\n",
    "    \"resolution\": resolution,\n",
    "    \"output_crs\": output_crs,\n",
    "}\n",
    "\n",
    "# Export query to pickle file for future re-use\n",
    "with open('results/query.pickle', 'wb') as f:\n",
    "    pickle.dump(query, f)\n",
    "    \n",
    "    \n",
    "# time_ranges = {\n",
    "#     \"Q3_2021\": slice(\"2021-08-01\", \"2022-10-31\"),\n",
    "#     \"Q4_2021\": slice(\"2021-11-01\", \"2022-01-31\"),\n",
    "#     \"Q1_2022\": slice(\"2022-02-01\", \"2022-04-30\"),\n",
    "# }\n",
    "# query.update({\"time_ranges\": time_ranges})\n",
    "\n",
    "# annual_geomedian_times = {\n",
    "#     \"annual_2021\": \"2021-01-01\",\n",
    "# }\n",
    "# semiannual_geomedian_times = {\n",
    "#     \"semiannual_2021_01\": \"2021-01-01\",\n",
    "#     \"semiannual_2021_06\": \"2021-06-01\",\n",
    "# }\n",
    "# query.update(\n",
    "#     {\n",
    "#         \"annual_geomedian_times\": annual_geomedian_times,\n",
    "#         \"semiannual_geomedian_times\": semiannual_geomedian_times,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed5f26-17d9-47b2-8158-5b1e09f3f7e9",
   "metadata": {},
   "source": [
    "## Collect training data\n",
    "\n",
    "By default, the method below will run in parallel mode, which decreases the amount of time to run feature extraction for each geometry. This will work well as long as your feature collection function (defined in feature_collection.py) is running with no problems. \n",
    "\n",
    "### When testing\n",
    "If you are testing a new feature collection function, it is suggested you set `parallel = False` below to switch back to serial mode. \n",
    "\n",
    "You can also set `gdf = single_crops_subset.iloc[0:5, :].copy()` in the function call to only run the first five geometries.\n",
    "\n",
    "### Features of this model\n",
    "\n",
    "* S2 BIMONTHLY GEOMEDIANS\n",
    "    * The geomedian is calculated for each two month period\n",
    "    * There are three periods: 2, 4, and 6 months prior to the collection date\n",
    "    * All bands + indices (NDVI, LAI, SAVI, MSAVI, MNDWI)\n",
    "* S2 Annual GEOMEDIAN\n",
    "    * The annual geomedian in the year prior to sampling\n",
    "    *  All bands + indices (NDVI, LAI, SAVI, MSAVI, MNDWI)\n",
    "* S1 BIMONTHLY GEOMEDIANS\n",
    "    * The geomedian is calculated for each two month period\n",
    "    * There are three periods: 2, 4, and 6 months prior to the collection date\n",
    "    * VV and VH bands\n",
    "* LANDSAT BIMONTHLY FRACTIONAL COVER\n",
    "    * The median is caluclated for each two month period\n",
    "    * There are three periods: 2, 4, and 6 months prior to the collection date\n",
    "    \n",
    "The total number of features from this collection is 73."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7239d8-1fa2-469b-a38c-ed39db1c9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parallel mode on or off (set to False if testing a new feature extraction function).\n",
    "parallel = True\n",
    "\n",
    "if parallel:\n",
    "    ncpus = round(get_cpu_quota())\n",
    "else:\n",
    "    ncpus = 1\n",
    "    \n",
    "print(\"ncpus = \" + str(ncpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dda35-d4d3-4d61-84ca-f1736c470ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the training data\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=single_crops_subset,\n",
    "    dc_query=query,\n",
    "    ncpus=ncpus,\n",
    "    field=\"label\",\n",
    "    zonal_stats=None,\n",
    "    feature_func=feature_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14f467-f012-48ae-828a-c2d3d5f0f82b",
   "metadata": {},
   "source": [
    "## Export training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349687e0-f375-4f87-97a8-20ed44711e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = f\"results/training_data_multipixel.txt\"\n",
    "\n",
    "#grab all columns\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names]\n",
    "\n",
    "#Export files to disk\n",
    "np.savetxt(output_file, model_input[:, model_col_indices], header=\" \".join(column_names), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90e66c-37ad-439e-bde6-1fbab3d16065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
