{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install richdem\n",
    "# !pip install https://packages.dea.ga.gov.au/hdstats/hdstats-0.1.5.tar.gz\n",
    "# !pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import subprocess as sp\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from odc.algo import xr_geomedian, int_geomedian\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_datahandling import load_ard\n",
    "from deafrica_spatialtools import xr_rasterize\n",
    "from deafrica_bandindices import calculate_indices\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "from deafrica_classificationtools import HiddenPrints\n",
    "from deafrica_plotting import rgb, display_map, map_shapefile\n",
    "from deafrica_classificationtools import HiddenPrints, predict_xr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell. You can use the dashboard to monitor the progress of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36807</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/chad/proxy/8787/status' target='_blank'>/user/chad/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>15</li>\n",
       "  <li><b>Memory: </b>104.37 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36807' processes=1 threads=15, memory=104.37 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `model_path`: The path to the location where the model exported from the previous notebook is stored\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `features_scaled`: Use this parameter to indicate whether or not the features where scaled using the `standardScalar()` method in the previous notebook\n",
    "* `sc_path`: If the `features_scaled` is set to `True`, then provide the path to the `standardScalar` values output in the previous notebook\n",
    "* `locations`: A dictionary with values containing latitude and longitude points, and keys representing a unique ID to indetify the locations (these keys are used as filenames when exporting classifications as geotiffs).\n",
    "* `buffer`: The number of degrees to load around the central latitude and longitude points in `locations`. This number here will depend on the compute/RAM available on the Sandbox instance, and the type and number of feature layers calculated. A value of `0.25` (which results in a 0.5 x 0.5 degree analysis extent) usually works well on the large sandbox instance.\n",
    "* `results`: A location to store the classified geotiffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results/ml_model.joblib'\n",
    "\n",
    "training_data = \"results/training_data/test_training_data.txt\"\n",
    "\n",
    "features_scaled = False\n",
    "sc_path = None #'results/std_scaler.bin'\n",
    "\n",
    "test_shapefile = 'data/testing_sites/eastern_testing_sites.geojson'\n",
    "\n",
    "results = 'results/classifications/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 15\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "except:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and inspect test_shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(test_shapefile)\n",
    "\n",
    "# gdf.head()\n",
    "# map_shapefile(gdf, attribute='GRID_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the model\n",
    "\n",
    "If we ran the optional feature scaling method in the `3_Train_fit_evaluate_classifier.ipynb`, then we will also load in the standard scalar values.\n",
    "\n",
    "The code below will also re-open the training data we exported from `2_Inspect_training_data.ipynb` and grab the column names (features we selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_path)\n",
    "\n",
    "if features_scaled:\n",
    "    sc=load(sc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red_S1', 'blue_S1', 'green_S1', 'nir_S1', 'swir_1_S1', 'edev_S1', 'bcdev_S1', 'NDVI_S1', 'LAI_S1', 'red_S2', 'blue_S2', 'green_S2', 'swir_1_S2', 'NDVI_S2', 'LAI_S2']\n"
     ]
    }
   ],
   "source": [
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[2:]\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Redefining the feature layer function\n",
    "\n",
    "How you define this function will depend on the steps you took in the previous notebooks.  If you elected to use the feature selection methods in `2_Inspect_training_data.ipynb` to reduce the number of features, then you may need to reconfigure the function to reduce the number of layers loaded (thereby reducing the time it takes to load and/or compute the feature layers). If you instead elected to use all the features extracted in `1_Extract_training_data.ipynb`, then you can simply copy-and-paste your `custom_training_data` function from the first notebook into the cell below.\n",
    "\n",
    "In the default example, we can remove the `slope` load/calculation because we identified that feature layer as less important.  We also add an extra line that selects only those features we earlier indentified as the best features, i.e.:\n",
    "                        \n",
    "                       result = result[column_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xr_geomedian_tmad import xr_geomedian_tmad\n",
    "\n",
    "def two_seasons_gm_mads(ds, column_names):\n",
    "    dc = datacube.Datacube(app='training')\n",
    "    ds = ds / 10000\n",
    "    ds1 = ds.sel(time=slice('2019-01', '2019-06'))\n",
    "    ds2 = ds.sel(time=slice('2019-07', '2019-12')) \n",
    "    \n",
    "    def fun(ds, era):\n",
    "        \n",
    "        #geomedian and tmads\n",
    "        gm_mads = xr_geomedian_tmad(ds)\n",
    "        gm_mads = calculate_indices(gm_mads,\n",
    "                               index=['NDVI', 'LAI'],\n",
    "                               drop=False,\n",
    "                               normalise=False,\n",
    "                               collection='s2')\n",
    "        \n",
    "        gm_mads['edev'] = -np.log(gm_mads['edev'])\n",
    "        gm_mads['sdev'] = -np.log(gm_mads['sdev'])\n",
    "        gm_mads['bcdev'] = -np.log(gm_mads['bcdev'])\n",
    "        \n",
    "        for band in gm_mads.data_vars:\n",
    "            gm_mads = gm_mads.rename({band:band+era})\n",
    "        \n",
    "        return gm_mads\n",
    "    \n",
    "    epoch1 = fun(ds1, era='_S1')\n",
    "    epoch2 = fun(ds2, era='_S2')\n",
    "    \n",
    "    result = xr.merge([epoch1,\n",
    "                       epoch2],compat='override')\n",
    "    \n",
    "    # keep only the features we identified as useful\n",
    "    # in the earlier notebooks\n",
    "    result = result[column_names]\n",
    "\n",
    "    return result.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up datacube query\n",
    "\n",
    "These query options should match the query params in `1_Extract_training_data.ipynb`, unless there are measurements that no longer need to be loaded because they were dropped during the feature selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products =  ['s2_l2a']\n",
    "time = ('2019-01','2019-12')\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "measurements =  ['red','blue','green','nir','swir_1']\n",
    "resolution = (-30,30)\n",
    "output_crs='epsg:6933'\n",
    "dask_chunks={'x':1000,'y':1000,'time':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loop through test tile and predict\n",
    "\n",
    "For every tile we list in the `test_shapefile`, we calculate the feature layers, and then use the DE Africa functions `predict_xr` and `predict_proba_xr` to classify the data.\n",
    "\n",
    "The results are exported to file as Cloud-Optimised Geotiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "features = []\n",
    "\n",
    "for index, row in gdf[0:2].iterrows():\n",
    "    \n",
    "    #get id for labelling\n",
    "    g_id=gdf.iloc[index]['GRID_ID']\n",
    "    print('working on grid: ' + g_id)\n",
    "    \n",
    "    # Get the geometry\n",
    "    geom = geometry.Geometry(row.geometry.__geo_interface__,\n",
    "                             geometry.CRS(f'EPSG:{gdf.crs.to_epsg()}'))\n",
    "\n",
    "     # generate a datacube query object\n",
    "    query = {\n",
    "        'time': time,\n",
    "        'measurements': measurements,\n",
    "        'resolution': resolution,\n",
    "        'output_crs': output_crs,\n",
    "        'group_by' : 'solar_day',\n",
    "    }\n",
    "    \n",
    "    # Update dc query with geometry      \n",
    "    query.update({'geopolygon': geom}) \n",
    "    \n",
    "    #load data\n",
    "    with HiddenPrints():\n",
    "        ds = load_ard(dc=dc,\n",
    "                      products=products,\n",
    "                      dask_chunks=dask_chunks,\n",
    "                      **query)\n",
    "        \n",
    "    #calculate features\n",
    "    data = two_seasons_gm_mads(ds, column_names)\n",
    "    \n",
    "    #predict using the imported model\n",
    "    predicted = predict_xr(model, data, proba=True, persist=True, clean=True).compute()\n",
    "    \n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    with HiddenPrints():\n",
    "        mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "        predicted = predicted.where(mask)\n",
    "    \n",
    "    predictions.append(predicted)\n",
    "    print('   loading features...')\n",
    "    features.append(data.compute())\n",
    "        \n",
    "    #export data to disk\n",
    "    write_cog(predicted.Predictions, results+ 'Eastern_tile_'+g_id+'_prediction.tif', overwrite=True)\n",
    "    write_cog(predicted.Probabilities, results+ 'Eastern_tile_'+g_id+'_probability.tif', overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(predictions)):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(30, 12))\n",
    "\n",
    "\n",
    "    # Plot classified image\n",
    "    predictions[i].Predictions.plot(ax=axes[0], \n",
    "                   cmap='Greens', \n",
    "                   add_labels=False, \n",
    "                   add_colorbar=False)\n",
    "\n",
    "    # Plot true colour image\n",
    "    rgb(features[i], bands=['red_S2','green_S2','blue_S2'],\n",
    "        ax=axes[1], percentile_stretch=(0.01, 0.99))\n",
    "\n",
    "    predictions[i].Probabilities.plot(ax=axes[2], \n",
    "                   cmap='magma',\n",
    "                   vmin=0,\n",
    "                   vmax=100,\n",
    "                   add_labels=False, \n",
    "                   add_colorbar=True)\n",
    "\n",
    "    # Remove axis on right plot\n",
    "    axes[2].get_yaxis().set_visible(False)\n",
    "\n",
    "    # Add plot titles\n",
    "    axes[0].set_title('Classified Image')\n",
    "    axes[1].set_title('True Colour Geomedian')\n",
    "    axes[2].set_title('Probabilities');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale classification\n",
    "\n",
    "If you're happy with the results of the test locations, then attempt to classify a large region by re-entering a new latitude, longitude and larger buffer size. You may need to adjust the `dask_chunks` size to optimize for the larger region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear objects from memory\n",
    "del ds\n",
    "del predictions\n",
    "del features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lat, new_lon = -0.2820, 37.0690\n",
    "new_buffer = 0.1\n",
    "dask_chunks={'x':1000,'y':1000,'time':1}\n",
    "name='Kenya'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_map((new_lon-new_buffer, new_lon+new_buffer), (new_lat+new_buffer, new_lat-new_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pixel quality parameters for Sentinel 2\n",
      "Finding datasets\n",
      "    s2_l2a\n",
      "Applying pixel quality/cloud mask\n",
      "Returning 70 time steps as a dask array\n",
      "   predicting...\n",
      "   probabilities...\n",
      "   input features...\n",
      "CPU times: user 4.49 s, sys: 438 ms, total: 4.93 s\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# generate a datacube query object\n",
    "query = {\n",
    "    'x': (new_lon-new_buffer, new_lon+new_buffer),\n",
    "    'y': (new_lat+new_buffer, new_lat-new_buffer),\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}\n",
    "\n",
    "#load data\n",
    "ds = load_ard(dc=dc,\n",
    "              products=products,\n",
    "              dask_chunks=dask_chunks,\n",
    "              **query)\n",
    "\n",
    "# calculate features (don't compute, let predict_xr do that)\n",
    "features = two_seasons_gm_mads(ds, column_names)\n",
    "\n",
    "#predict using the imported model\n",
    "predicted = predict_xr(model, features, proba=True, persist=True, clean=True, return_input=True).compute()\n",
    "\n",
    "# write_cog(predicted.Predictions, results+ 'Eastern_tile_'+name+'_prediction.tif', overwrite=True)\n",
    "# write_cog(predicted.Probabilities, results+ 'Eastern_tile_'+name+'_probabilities.tif', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(30, 15))\n",
    "\n",
    "# Plot classified image\n",
    "predicted.Predictions.plot(ax=axes[0], \n",
    "               cmap='Greens', \n",
    "               add_labels=False, \n",
    "               add_colorbar=False)\n",
    "\n",
    "predicted.Probabilities.plot(ax=axes[1], \n",
    "               cmap='magma',\n",
    "               vmin=0,\n",
    "               vmax=100,\n",
    "               add_labels=False, \n",
    "               add_colorbar=True)\n",
    "\n",
    "Remove axis on right plot\n",
    "axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "Add plot titles\n",
    "axes[0].set_title('Classified Image')\n",
    "axes[1].set_title('Probabilities');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# CUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #band indices\n",
    "#         bi = calculate_indices(ds,\n",
    "#                                index=['EVI', 'LAI'],\n",
    "#                                drop=True,\n",
    "#                                normalise=False,\n",
    "#                                collection='s2')\n",
    "        \n",
    "#         bi_max = bi.max('time')\n",
    "#         bi_min = bi.min('time')\n",
    "#         bi_range = bi_max - bi_min\n",
    "#         bi_range = bi_range.rename({'EVI':'EVI_range','LAI':'LAI_range'})\n",
    "#         bi_max = bi_max.rename({'EVI':'EVI_max','LAI':'LAI_max'})\n",
    "#         bi_min = bi_min.rename({'EVI':'EVI_min','LAI':'LAI_min'})\n",
    "#  out = xr.merge([gm_mads,bi_max,bi_min,bi_range],compat='override')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_locations = {\n",
    "#              '1':(12.2636, 37.0244),\n",
    "#              '2':(-0.2953, 38.4422),\n",
    "#              '3':(11.5217, 37.4894),\n",
    "#              '4':(-3.2838, 35.8406),\n",
    "#              '5':(-0.3878, 37.4869),\n",
    "#              '6':(-3.4866, 37.3650),\n",
    "#              '7':(-0.7062, 36.5865),\n",
    "#              '8':(-0.6926, 35.6443)\n",
    "#             }\n",
    "\n",
    "# buffer = 0.15\n",
    "\n",
    "\n",
    "# for key, latlon in test_locations.items():\n",
    "#     print(\"Working on : \"+str(latlon))\n",
    "    \n",
    "#     # generate a datacube query object\n",
    "#     query = {\n",
    "#         'x': (latlon[1]-buffer, latlon[1]+buffer),\n",
    "#         'y': (latlon[0]+buffer, latlon[0]-buffer),\n",
    "#         'time': time,\n",
    "#         'measurements': measurements,\n",
    "#         'resolution': resolution,\n",
    "#         'output_crs': output_crs,\n",
    "#         'group_by' : 'solar_day',\n",
    "#     }\n",
    "    \n",
    "#     #load data\n",
    "#     with HiddenPrints():\n",
    "#         ds = load_ard(dc=dc,\n",
    "#                       products=products,\n",
    "#                       dask_chunks=dask_chunks,\n",
    "#                       min_gooddata=0.10,\n",
    "#                       **query)\n",
    "        \n",
    "#     # calculate features\n",
    "#     data = two_seasons_gm_mads(ds, column_names)\n",
    "    \n",
    "#     #predict using the imported model\n",
    "#     predicted = predict_xr(model, data, proba=True, persist=True, clean=True).compute()\n",
    "#     predictions.append(predicted)\n",
    "#     print('   loading features...')\n",
    "#     features.append(data.compute())\n",
    "        \n",
    "#     #export data to disk\n",
    "#     write_cog(predicted.Predictions, results+ 'Eastern_tile_'+g_id+'_prediction.tif', overwrite=True)\n",
    "#     write_cog(predicted.Probabilities, results+ 'Eastern_tile_'+g_id+'_probability.tif', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
