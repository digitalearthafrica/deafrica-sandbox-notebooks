{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating training data on the ODC <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:** \n",
    "[ga_ls8c_gm_2_annual](https://explorer.digitalearth.africa/ga_ls8c_gm_2_annual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**Training data** is the most important part of any supervised machine learning workflow. The quality of the training data has a greater impact on the classification than the algorithm used. Large and accurate training data sets are preferable: increasing the training sample size results in increased classification accuracy ([Maxell et al 2018](https://www.tandfonline.com/doi/full/10.1080/01431161.2018.1433343)).\n",
    "\n",
    "A review of training data methods in the context of Earth Observation is available [here](https://www.mdpi.com/2072-4292/12/6/1034) \n",
    "\n",
    "When creating training data, be sure to capture the **spectral variability** of the class, and to use imagery from the time period you want to classify (rather than relying on basemap composites). Another common problem with training data is **class imbalance**. This can occur when one of your classes is relatively rare and therefore the rare class will comprise a smaller proportion of the training set. When imbalanced data is used, it is common that the final classification will under-predict less abundant classes relative to their true proportion.\n",
    "\n",
    "There are many platforms to use for gathering training data, the best one to use depends on your application. GIS platforms are great for collection training data as they are highly flexible platforms; [geo-wiki](https://www.geo-wiki.org/) and [Collect Earth Online](https://collect.earth/home) are two open-source websites that also very useful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook will extract training data from the `open-data-cube` using geometries within a shapefile (or geojson). To do this, we rely on a custom `deafrica-sandbox-notebooks` function called `collect_training_data`, contained within the [deafrica_classificationtools](../Scripts/deafrica_classificationtools.py) script.  The goal of this notebook is to familarise users with this function so they can extract the appropriate data for their use-case.\n",
    "\n",
    "1. First we do this\n",
    "2. Then we do this\n",
    "3. Finally we do this\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_classificationtools import collect_training_data \n",
    "from deafrica_bandindices import calculate_indices\n",
    "from deafrica_temporal_statistics import temporal_statistics\n",
    "from deafrica_plotting import map_shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `path`: The path to the input shapefile from which we will extract training data. A default shapefile is provided.\n",
    "* `field`: This is the name of column in your shapefile attribute table that contains the class labels. The class labels must be integers\n",
    "* `ncpus`: Set this value to > 1 to parallize the collection of training data. eg. npus=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 2\n"
     ]
    }
   ],
   "source": [
    "path = 'data/training/test_training_dataset.shp' \n",
    "field = 'class'\n",
    "\n",
    "# automatically detect number of cpus, adjust to [-4:] if working on XXL Sandbox\n",
    "ncpus= int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview input data\n",
    "\n",
    "We can load and preview our input data shapefile using `geopandas`. The shapefile should contain a column with class labels (e.g. 'class'). These labels will be used to train our model. Remember, the class labels must be represented by integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ID</th>\n",
       "      <th>CODE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>BOT</td>\n",
       "      <td>Southern</td>\n",
       "      <td>POLYGON ((19.76605 -34.71434, 19.76605 -34.713...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>BOT</td>\n",
       "      <td>Southern</td>\n",
       "      <td>POLYGON ((20.17941 -34.21388, 20.17941 -34.213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>BOT</td>\n",
       "      <td>Southern</td>\n",
       "      <td>POLYGON ((21.35220 -33.78466, 21.35220 -33.783...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>BOT</td>\n",
       "      <td>Southern</td>\n",
       "      <td>POLYGON ((25.72613 -33.69673, 25.72613 -33.695...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>BOT</td>\n",
       "      <td>Southern</td>\n",
       "      <td>POLYGON ((21.78891 -33.53318, 21.78891 -33.532...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  ID CODE   COUNTRY                                           geometry\n",
       "0      1  21  BOT  Southern  POLYGON ((19.76605 -34.71434, 19.76605 -34.713...\n",
       "1      1  21  BOT  Southern  POLYGON ((20.17941 -34.21388, 20.17941 -34.213...\n",
       "2      0  21  BOT  Southern  POLYGON ((21.35220 -33.78466, 21.35220 -33.783...\n",
       "3      0  21  BOT  Southern  POLYGON ((25.72613 -33.69673, 25.72613 -33.695...\n",
       "4      0  21  BOT  Southern  POLYGON ((21.78891 -33.53318, 21.78891 -33.532..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input data shapefile\n",
    "input_data = gpd.read_file(path)\n",
    "\n",
    "# Plot first five rows\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dffb7f44d1408bb2809287d6b8a8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc4d35a7d344e82b2792dc16b4d5154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-22.12386984762135, 26.34933471312903], controls=(ZoomControl(options=['position', 'zoom_in_text',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training data in an interactive map\n",
    "map_shapefile(input_data, attribute=field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect training data\n",
    "\n",
    "The function `collect_training_data` takes our shapefile containing class labels and extracts training data from the datacube over the location specified by the input geometries. The function will also pre-process our training data by stacking the arrays into a useful format and removing an `NaN` (not-a-number) values. \n",
    "\n",
    "`Collect_training_data` has the ability to generate many different types of **feature layers**, relatively simple layers can be calculated using pre-defined parameters within the function; more complex layers can be computed by passing in a `custom_func`. To begin with, let's try generating feature layers using the pre-defined methods.\n",
    "\n",
    "The in-built feature layer parameters are described below:\n",
    "* `product`: The name of the product to extract from the datacube. In this example we use a geomedian composite from 2018, `'ga_ls8c_gm_2_annual'`\n",
    "* `time`: The time range over to extract data\n",
    "* `calc_indices`: This parameter provides a method for calculating a number of remote sensing indices (e.g. `['NDWI', 'NDVI']`). Any of the indices found in the [deafrica_bandindices](../Scripts/deafrica_bandindices.py) script can be used here\n",
    "* `reduce_func`: Function to reduce the data from multiple time steps to a single timestep. Options are 'mean', 'median', 'std', 'max', 'min', 'geomedian'. In the default example we are loading a geomedian composite, so there is not time dimension to reduce.\n",
    "* `drop`: If this variable is set to True, and 'calc_indices' are supplied, the spectral bands will be dropped from the dataset leaving only the band indices as data variables in the dataset.\n",
    "* `zonal_stats`: An optional string giving the names of zonal statistics to calculate for each polygon. Default is None (all pixel values are returned). Supported values are 'mean', 'median', 'max', 'min', and 'std'.\n",
    "\n",
    "In addition to the parameters required for `collect_training_data`, we also need to set up a few parameters for the open-data-cube query, such as `measurements` (the bands to load from the satellite), the `resolution` (the cell size), and the `output_crs` (the output projection). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products =  ['ga_ls8c_gm_2_annual']\n",
    "time = ('2018')\n",
    "reduce_func = None\n",
    "calc_indices = ['EVI', 'LAI'] \n",
    "drop = False\n",
    "zonal_stats = 'median' \n",
    "\n",
    "# Set up the inputs for the ODC ODC query\n",
    "measurements =  ['blue','green','red','nir','swir_1','swir_2']\n",
    "resolution = (-20,20)\n",
    "output_crs='epsg:6933'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization, however, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization. \n",
    "\n",
    "Now let's run the `collect_training_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating indices: ['EVI', 'LAI']\n",
      "Taking zonal statistic: median\n",
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [04:40<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output training data has shape (292, 9)\n",
      "Removed NaNs, cleaned input shape:  (292, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "column_names, model_input = collect_training_data(\n",
    "                                    gdf=input_data,\n",
    "                                    products=products,\n",
    "                                    dc_query=query,\n",
    "                                    ncpus=ncpus,\n",
    "                                    field=field,\n",
    "                                    calc_indices=calc_indices,\n",
    "                                    reduce_func=reduce_func,\n",
    "                                    drop=drop,\n",
    "                                    zonal_stats=zonal_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns two numpy arrays, the first (`column_names`) contains a list of the names of the feature layers we've computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'blue', 'green', 'red', 'nir', 'swir_1', 'swir_2', 'EVI', 'LAI']\n"
     ]
    }
   ],
   "source": [
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second array (`model_input`) contains the data from our labelled geometries. The first item in the array is the class integer (e.g. in the default example 1. or 0.), the second set of items are the values for each feature layer we computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.    0.11  0.18 ...  0.35  0.   -0.12]\n",
      " [ 1.    0.07  0.09 ...  0.23  0.   -0.12]\n",
      " [ 0.    0.03  0.05 ...  0.13  0.   -0.12]\n",
      " ...\n",
      " [ 1.    0.06  0.1  ...  0.24  0.   -0.12]\n",
      " [ 0.    0.05  0.08 ...  0.16  0.   -0.12]\n",
      " [ 0.    0.04  0.06 ...  0.14  0.   -0.12]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom feature layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = \"training_data.txt\"\n",
    "# np.savetxt(output_file, model_input, header=\" \".join(column_names), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = os.path.join(working_dir, '2010_2015_median_indices_training_data_binary.txt')\n",
    "# model_input = numpy.loadtxt(filename, skiprows=1)\n",
    "# random_state = 1234\n",
    "# # Set up header and input features\n",
    "# with open(filename, 'r') as file:\n",
    "#     header = file.readline()\n",
    "# column_names = header.split()\n",
    "# column_names_indices = {}\n",
    "# for col_num, var_name in enumerate(column_names):\n",
    "#     column_names_indices[var_name] = col_num\n",
    "# model_variables = ['blue','red','green','nir','swir1','swir2','edev','sdev','bcdev', 'NDVI', 'MNDWI', 'BAI', 'BUI', 'BSI', 'TCG', 'TCW', 'TCB', 'NDMI', 'LAI', 'EVI', 'AWEI_sh', 'BAEI', 'NDSI', 'SAVI']\n",
    "# model_col_indices = []\n",
    "# for model_var in model_variables:\n",
    "#     model_col_indices.append(column_names_indices[model_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended next steps\n",
    "\n",
    "To continue working through the notebooks in this `Scalable Machine Learning on the ODC` workflow, go to the next notebook `Training data knowledge base`\n",
    "\n",
    "1. **1_Generate_training_data (this notebook)**\n",
    "2. [Training data knowledge base]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** August 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**:  :index:`Sentinel-2`, :index:`deafrica_classificationtools`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
