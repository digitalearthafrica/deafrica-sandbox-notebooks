{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, fit, and evaluate classifier <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "\n",
    "TODO:\n",
    "- if/when datasets become large, consider implementing `dask_ml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "\n",
    "\n",
    "_Table 1: Some of the pros and cons of different classifiers available through scikit-learn_\n",
    "\n",
    "<img align=\"center\" src=\"classifier_pro_cons.PNG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "1. Split the training data into a training set and a test set\n",
    "2. Optionally standardise the datasets\n",
    "2. Train a classifier\n",
    "3. Evaluate the classifier using a number of metrics\n",
    "4. Optimise the model hyperparameters\n",
    "5. Retrain the model using the optimised hyperparameters\n",
    "6. Re-evaluate the classifier using a number of metrics\n",
    "7. Optionally plot Receiver Operating Characteristic (ROC) Curves (for binary classification only)\n",
    "6. Save model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask-ml\n",
    "# !pip freeze | grep hdstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scikit-learn classifiers, uncomment the one of interest----\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "import subprocess as sp\n",
    "import dask.array as da\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_classificationtools import spatial_clusters, SKCV, spatial_train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `Classifier`: This parameter refers to the scikit-learn classification model to use, first uncomment the classifier of interest in the `Load Packages` section and then enter the function name into this parameter `e.g. Classifier = SVC`   \n",
    "* `class_dict`: A dictionary mapping the 'string' name of the classes to the integer values that represent our classes in the training data (e.g. `{'crop': 1., 'noncrop': 0.}`)\n",
    "* `metric` : A single string that denotes the scorer used to find the best parameters for refitting the estimator to evaluate the predictions on the test set. See the scoring parameter page [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for a pre-defined list of options.\n",
    "* `inner_cv_splits` : \n",
    "* `outer_cv_splits` :\n",
    "\n",
    "### Spatial K-Fold Cross Validation Parameters\n",
    "\n",
    "The deafrica function `SKCV` (spatial k-fold cross validation) is a custom function similar to `sklearn.model_selection.KFold` but instead works on spatial coordinate data.  Coordinate data is grouped according to either a Guassian Mixture, KMeans, or Heirachical clustering algorithm. Grouping by spatial clusters is preferred over plain random splits for spatial data to avoid overestimating validation scores due to the inherent autocorrelation of spatial data. To use `SKCV` we need to set more parameters: \n",
    "\n",
    "* `test_size` : This will determine what fraction of the dataset will be set aside as the testing dataset. There is a trade-off here between having a larger test set that will help us better determine the quality of our classifier, and leaving enough data to train the classifier. A good deafult is to set 10-20 % of your dataset aside for testing purposes.\n",
    "* `cluster_method` : Which algorithm to use to create spatial groups, either `'Hierarchical'`, `'GMM'` or `'KMeans'`. Key word arguments for these algorithms can also be passed to the function if non-default behaviour is required. See the docs for [GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html), [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [Heirarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) for options. \n",
    "* `max_distance`:  This parameter is used when using the 'Heirarchical' clustering method. The maximum distance describes the maximum euclidean distances between all observations in a cluster. The units of distance depend on the map projection of the coordinate data. e.g if using a UTM projection, then the `max_distance` is in metres, so to set the maximum distance of each cluster to 200 km, `max_distance=200000`. \n",
    "* `n_clusters` : Number of spatial clusters to create using the coordinate values of the training data samples. This option only applies if using the 'GMM' or 'Kmeans' clustering methods, if using the 'Heirarchical' method then the number of clusters is determined automatically using the maximum distance threshold.\n",
    "* `kfold_method` : Which stratgey to use to split to the data. One of either `'SpatialShuffleSplit'` or `'SpatialKFold'`.\n",
    "* `balance` : if setting `kfold_method` to `'SpatialShuffleSplit'` this should be an integer (10 is a good deafult) that represents the number of splits generated per iteration to try to balance the amount of data in each set so that *test_size* and *train_size* are respected. If setting `kfold_method` to `'SpatialKFold'` then this value should be either `True` or `False`.  If False, each fold will have the same number of clusters (which can have different number of data points in them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"results/training_data/test_training_data.txt\"\n",
    "coordinate_data = \"results/training_data/training_data_coordinates.txt\"\n",
    "\n",
    "Classifier = RandomForestClassifier\n",
    "\n",
    "class_dict = {'crop':1, 'noncrop':0}\n",
    "\n",
    "metric = 'f1' #good for binary classifications\n",
    "\n",
    "inner_cv_splits = 5\n",
    "\n",
    "outer_cv_splits = 10\n",
    "\n",
    "test_size = 0.15\n",
    "\n",
    "cluster_method = 'Hierarchical'\n",
    "\n",
    "max_distance = 200000\n",
    "\n",
    "n_clusters=None\n",
    "\n",
    "kfold_method = 'SpatialShuffleSplit'\n",
    "\n",
    "balance = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "except:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training  and coordinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "coordinates = np.loadtxt(coordinate_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:]\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]\n",
    "\n",
    "#convert variable names into sci-kit learn nomenclature\n",
    "X = model_input[:, model_col_indices]\n",
    "y = model_input[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the parameters, lets first generate spatial clusters to visualize how our data will be grouped for the SKCV.  You may want to refine the parameters to achieve a grouping that works for your dataset resetting the parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clustes\n",
    "spatial_groups = spatial_clusters(coordinates=coordinates,\n",
    "                                  method=cluster_method,\n",
    "                                  max_distance=max_distance,\n",
    "                                  n_groups=n_clusters)\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(6,8))\n",
    "plt.scatter(coordinates[:, 0], coordinates[:, 1], c=spatial_groups,\n",
    "            s=50, cmap='viridis');\n",
    "plt.title('Spatial clusters of training data')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate an unbiased performance estimate via nested cross-validation.  \n",
    "\n",
    "https://jmlr.csail.mit.edu/papers/v11/cawley10a.html\n",
    "\n",
    "https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/\n",
    "\n",
    "\n",
    "<img align=\"center\" src=\"nested_CV.png\" width=\"500\">\n",
    "\n",
    "\n",
    "> **Note**: the parameters in the `param_grid` object depend on the classifier being used. The default example is set up for a random forest classifier, to adjust ther paramaters to suit a different classifier, look up the important parameters under the relevant [sklearn documentation](https://scikit-learn.org/stable/supervised_learning.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'class_weight': ['balanced', None],\n",
    "    'max_features': ['auto', 'log2', None],\n",
    "    'n_estimators': [100,200,300,400,500],\n",
    "    'criterion':['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outer k-fold splits\n",
    "outer_cv = SKCV(\n",
    "    coordinates=coordinates,\n",
    "    max_distance=max_distance,\n",
    "    n_splits=outer_cv_splits,\n",
    "    cluster_method=cluster_method,\n",
    "    kfold_method=kfold_method,\n",
    "    test_size=test_size,\n",
    "    balance=balance,\n",
    ")\n",
    "\n",
    "# lists to store results of CV testing\n",
    "acc = []\n",
    "f1 = []\n",
    "roc_auc = []\n",
    "\n",
    "#intiate a model\n",
    "model = Classifier(random_state=1)\n",
    "\n",
    "# loop through outer splits and test predictions\n",
    "for train_index, test_index in outer_cv.split(coordinates):\n",
    "\n",
    "    # index training, testing, and coordinate data\n",
    "    X_tr, X_tt = X[train_index, :], X[test_index, :]\n",
    "    y_tr, y_tt = y[train_index], y[test_index]\n",
    "    coords = coordinates[train_index]\n",
    "\n",
    "    # inner split on data within outer split\n",
    "    inner_cv = SKCV(\n",
    "        coordinates=coords,\n",
    "        max_distance=max_distance,\n",
    "        n_splits=inner_cv_splits,\n",
    "        cluster_method=cluster_method,\n",
    "        kfold_method=kfold_method,\n",
    "        test_size=test_size,\n",
    "        balance=balance,\n",
    "    )\n",
    "    \n",
    "    #perfrom grid search on hyperparameters\n",
    "    clf = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=metric,\n",
    "        n_jobs=ncpus,\n",
    "        cv=inner_cv.split(coords),\n",
    "        refit=True,\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_tr, y_tr)\n",
    "    #predict using the best model\n",
    "    best_model = clf.best_estimator_\n",
    "    pred = best_model.predict(X_tt)\n",
    "\n",
    "    # evaluate model w/ multiple metrics\n",
    "    # ROC AUC\n",
    "    probs = best_model.predict_proba(X_tt)\n",
    "    probs = probs[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_tt, probs)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    roc_auc.append(auc_)\n",
    "    # Overall accuracy\n",
    "    ac = accuracy_score(y_tt, pred)\n",
    "    acc.append(ac)\n",
    "    # F1 scores\n",
    "    f1_ = f1_score(y_tt, pred)\n",
    "    f1.append(f1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters\n",
    "\n",
    "Hyperparameter searches are a required process in machine learning. Machine learning models require certain “hyperparameters”, model parameters that can be tuned to increase the prediction ability of a model. Finding the best values for these parameters is a “hyperparameter search” or an “hyperparameter optimization.”\n",
    "\n",
    "To optimize the parameters in our model, we use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to exhaustively search through a set of parameters and determine the combination that will result in the highest accuracy based upon the accuracy metric defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n_splits of train-test_split\n",
    "ss = SKCV(\n",
    "        coordinates=coordinates,\n",
    "        max_distance=max_distance,\n",
    "        n_groups=n_clusters,\n",
    "        n_splits=outer_cv_splits,\n",
    "        cluster_method=cluster_method,\n",
    "        kfold_method=kfold_method,\n",
    "        test_size=test_size,\n",
    "        balance=balance\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instatiate a gridsearchCV\n",
    "clf = GridSearchCV(Classifier(),\n",
    "                   param_grid,\n",
    "                   scoring=metric,\n",
    "                   verbose=1,\n",
    "                   cv=ss.split(coordinates),\n",
    "                   n_jobs=ncpus)\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print('\\n')\n",
    "print(\"The \"+metric+\" score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model\n",
    "\n",
    "Using the best parameters from our hyperparmeter optmization search, we now fit our model on all the data to give the best possible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new model\n",
    "new_model = Classifier(**clf.best_params_, random_state=1, n_jobs=ncpus)\n",
    "new_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "Running this cell will export the classifier as a binary`.joblib` file. This will allow for importing the model in the subsequent script, `4_Predict.ipynb` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(new_model, 'results/ml_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended next steps\n",
    "\n",
    "To continue working through the notebooks in this `Scalable Machine Learning on the ODC` workflow, go to the next notebook `4_Predict.ipynb`.\n",
    "\n",
    "1. [Extracting_training_data](1_Extracting_training_data.ipynb) \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. **Train_fit_evaluate_classifier**\n",
    "4. [Predict](4_Predict.ipynb)\n",
    "5. [Accuracy_assessment](5_Accuracy_assessment.ipynb)\n",
    "6. [Object-based_filtering](6_Object-based_filtering_(optional).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** August 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
