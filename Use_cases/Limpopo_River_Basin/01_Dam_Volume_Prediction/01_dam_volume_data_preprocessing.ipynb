{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling dam volumes using DE Africa waterbodies\n",
    "# Section 01  : *Data Preprocessing*\n",
    "\n",
    "**Products used:** \n",
    "[DE Africa Waterbodies](https://docs.digitalearthafrica.org/en/latest/data_specs/Waterbodies_specs.html), \n",
    "[Department of Water Affairs and Sanitation, South Africa Dam Level and Volume Data](https://www.dws.gov.za/Hydrology/Verified/hymain.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Digital Twin (DT)\n",
    "The CGIAR Digital Twin initiative creates dynamic virtual models that combine real-time data, AI, and simulations to improve decision-making. Its prototype for the Limpopo River Basin focuses on enhancing water resource management and conservation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Description\n",
    "This notebook presents a workflow for predicting dam levels and volumes using water surface area data from DE Africa's Waterbodies product, integrating data preprocessing, feature extraction, and Gradient Boosting modeling. \n",
    "\n",
    "As part of the CGIAR Initiative on Digital Innovation, this work contributes to a prototype Digital Twin for the Limpopo River Basin, designed to support real-time decision-making in water management. The Digital Twin leverages AI-driven tools to visualize and simulate the impact of decisions on the basin's ecosystem. To enhance prediction reliability, the model includes a correction mechanism to address unrealistic large drops in dam volume estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import datacube\n",
    "import joblib\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from deafrica_tools.waterbodies import get_waterbody, get_time_series, display_time_series,get_geohashes\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "This section defines the analysis parameters, including:\n",
    "* `waterbody_geohash`: Unique identifier (uid) / [geohash](https://en.wikipedia.org/wiki/Geohash) a waterbody.  The geohash of a water body is derived from its position, and this process can be reversed to obtain the location from the geohash. A waterbody's geohash is contained under the `uid` attribute and can be obtained through DE Africa Maps by clicking on a [waterbody](https://maps.digitalearth.africa/waterbody).\n",
    "\n",
    "For this model development we will train and test for Loskop dam in South Africa with a DE Africa Water Body Geohash of:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_geohash = \"kekz70yc3g\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for a specific waterbody\n",
    "\n",
    "The returned GeoDataFrame includes the metadata for the selected waterbody including the id, uid, wb_id, area, perimeter and length. See the [Waterbodies Historical Extent documentation](https://docs.digitalearthafrica.org/en/latest/data_specs/Waterbodies_specs.html#Waterbodies-Historical-Extent) for descriptions of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody = get_waterbody(waterbody_geohash)\n",
    "waterbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the water body polygon is in memory, you can plot them directly, or explore them in an interactive window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the wet surface area time series for the selected waterbody\n",
    "\n",
    "For any given waterbody, we can also use the `get_time_series()` function to get various measures of the water body surface over time. See the [Waterbodies Historical Extent documentation](https://docs.digitalearthafrica.org/en/latest/data_specs/Waterbodies_specs.html#Waterbodies-Surface-Area-Change) for descriptions of the different surface measures.\n",
    "\n",
    "The function also calculates a rolling median of the water body surface wet percentage. This is used to visualise the overall trend in the surface wet percentage. The rolling median uses the last three observations to determine the median at a given date.\n",
    "\n",
    "> By default the entire timeseries for the waterbody i.e. `start_date=1984-01-01` and `end_date=today` is loaded for the waterbody by the `get_time_series()` function  unless a filter is applied by passing the `start_date` and `end_date` parameters to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_timeseries = get_time_series(waterbody=waterbody)\n",
    "waterbody_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the wet surface area time series for the selected waterbody\n",
    "\n",
    "After loading the water body time series, we can use the `display_time_series()` function to create an interactive visualisation of the time series.\n",
    "\n",
    "The visualisation shows the invalid percentage and the wet percentage. The invalid percentage indicates the proportion of the water body that couldn't be clearly observed. To provide the most representative measurements of water body surface area, the time series only contains values where the invalid percentage is lower than 10%.\n",
    "\n",
    "There are some caveats to be aware of:\n",
    "\n",
    "- To appear in the time series, an observation must record clear observations of at least 90% of the water body's surface area. If 10% or more of the surface area is covered by cloud or cloud shadow, the observation will be excluded. This can cause large gaps in the time series.\n",
    "- If the invalid percentage is high, it's likely that the wet percentage is an underestimate of the true wet surface area.\n",
    "- Annual and seasonal trends should only be inferred during times with sufficient observations. You should take care when infering the surface water change across time when there are few observations.\n",
    "- The time series is based on the Water Observations from Space product, which has known limitations. See the [DE Africa Waterbodies service documentation](https://docs.digitalearthafrica.org/en/latest/data_specs/Waterbodies_specs.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame with the index reset\n",
    "waterbody_timeseries_plot = waterbody_timeseries.reset_index()\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Plot Wet Percentage\n",
    "ax.scatter(waterbody_timeseries_plot['date'], waterbody_timeseries_plot['percent_wet'], \n",
    "           color='blue', label='Wet Percentage', s=20, marker='o')\n",
    "\n",
    "# Plot Invalid Percentage\n",
    "ax.scatter(waterbody_timeseries_plot['date'], waterbody_timeseries_plot['percent_invalid'], \n",
    "           color='red', label='Invalid Percentage', s=20, marker='o')\n",
    "\n",
    "# Plot Rolling Median Wet Percentage\n",
    "ax.plot(waterbody_timeseries_plot['date'], waterbody_timeseries_plot['percent_wet_rolling_median'], \n",
    "        color='blue', linewidth=1, label='Wet Percentage - Rolling Median')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_title('Wet Surface Area Time Series')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Percentage')\n",
    "plt.xticks(rotation=45)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.legend(title='Legend', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "# image_path = \"wet_surface_area_time_series_matplotlib.png\"\n",
    "# plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert waterbody_timeseries into hectares based datatable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hectare_table(waterbody_timeseries):\n",
    "    # Create water_area_ha by dividing area_wet_m2 by 10,000 (conversion from square meters to hectares)\n",
    "    waterbody_timeseries['water_area_ha'] = waterbody_timeseries['area_wet_m2'] / 10000\n",
    "\n",
    "    # Prepare final table\n",
    "    digital_twin_table = pd.DataFrame({\n",
    "        'original_id': ['kekz70yc3g'] * len(waterbody_timeseries),\n",
    "        'id': range(183875, 183875 + len(waterbody_timeseries)),\n",
    "        'waterbody_id': [518] * len(waterbody_timeseries),  # Assuming a constant waterbody_id as per Limpopo Digital Twin\n",
    "        'date': waterbody_timeseries.index,  # Assuming 'date' is the index of the waterbody_timeseries\n",
    "        'water_area_ha': waterbody_timeseries['water_area_ha'],\n",
    "        'percent_invalid': waterbody_timeseries['percent_invalid']\n",
    "    })\n",
    "    return digital_twin_table\n",
    "\n",
    "# Call the conversion function on the waterbody_timeseries to replace the old water levels data loading\n",
    "water_areas_ha = convert_to_hectare_table(waterbody_timeseries)\n",
    "water_areas_ha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sample Data Overview\n",
    "\n",
    "This dataset contains raw water levels data collected from DEA (Department of Environmental Affairs) in South Africa. The cell below reads this ancillary data necessary to conduct the volume prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_attributes = pd.read_csv('data/dam_attributes/dam_attributes.csv')\n",
    "rating_curve = pd.read_csv('data/DWS/rating_curve.csv')\n",
    "daily_volumes = pd.read_csv('data/DWS/daily_volumes.csv')\n",
    "\n",
    "dam_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract dam attributes\n",
    "The full value indicator i.e. the value representing the dam at 100% capacity for both the volume and the area were found to be a strong feature for training the model and acting as a baseline starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_volume = dam_attributes[dam_attributes['parameter'] == 'full_volume']['value'].values[0]\n",
    "full_surface_area = dam_attributes[dam_attributes['parameter'] == 'full_surface_area']['value'].values[0]\n",
    "full_dam_level = dam_attributes[dam_attributes['parameter'] == 'full_dam_level']['value'].values[0]\n",
    "maximum_dam_level = dam_attributes[dam_attributes['parameter'] == 'maximum_dam_level']['value'].values[0]\n",
    "capacity_of_spillway = dam_attributes[dam_attributes['parameter'] == 'capacity_of_spillway']['value'].values[0]\n",
    "vertical_drop = dam_attributes[dam_attributes['parameter'] == 'vertical_drop']['value'].values[0]\n",
    "mean_depth = dam_attributes[dam_attributes['parameter'] == 'mean_depth']['value'].values[0]\n",
    "full_capacity_elevation = dam_attributes[dam_attributes['parameter'] == 'full_capacity_elevation']['value'].values[0]\n",
    "shoreline_length = dam_attributes[dam_attributes['parameter'] == 'shoreline_length']['value'].values[0]\n",
    "maximum_volume = dam_attributes[dam_attributes['parameter'] == 'maximum_volume']['value'].values[0]\n",
    "maximum_surface_area = dam_attributes[dam_attributes['parameter'] == 'maximum_surface_area']['value'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data preprocessing stage, it was crucial to handle potential outliers in the water area dataset, which could negatively impact model accuracy. We applied the **Interquartile Range (IQR) method** to systematically identify and remove these outliers.\n",
    "\n",
    "The IQR method flags data points that lie beyond 1.5 times the interquartile range (IQR), calculated as the distance between the first quartile (Q1) and the third quartile (Q3). Any values falling below Q1 or above Q3 by more than 1.5 times the IQR were considered outliers. Removing these extreme values helped ensure that the model was trained on reliable, realistic water area measurements, improving its robustness and generalizability.\n",
    "\n",
    "To account for seasonal variability, the IQR method was applied on a per-month basis, ensuring that seasonal changes did not distort the detection process.\n",
    "\n",
    "**Key steps:**\n",
    "- Remove outliers per month using the IQR method.\n",
    "- Handle missing values by dropping rows containing `NaN` in critical feature columns, such as `water_area_ha`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter water levels based on percentage invalid\n",
    "- Filter out rows where the percent_invalid is greater than 10.\n",
    "- Calculate and print the number of samples removed during the filtering process.\n",
    "- Print the number of samples remaining in the dataset after the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count = len(water_areas_ha)\n",
    "water_areas_filtered = water_areas_ha[water_areas_ha['percent_invalid'] <= 0.5]\n",
    "final_count = len(water_areas_filtered)\n",
    "removed_count = initial_count - final_count\n",
    "print(f\"Number of samples removed: {removed_count}\")\n",
    "print(f\"Number of samples remaining: {final_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date columns to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_areas_filtered = water_areas_filtered.copy()\n",
    "daily_volumes = daily_volumes.copy()\n",
    "water_areas_filtered['date'] = pd.to_datetime(water_areas_filtered['date'])\n",
    "daily_volumes['Date'] = pd.to_datetime(daily_volumes['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers per month using the IQR method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the data preprocessing step, it is crucial to remove outliers that could negatively impact the accuracy of the model. To do this, the **Interquartile Range (IQR) method** is applied. Outliers are detected and removed if they lie beyond 1.5 times the interquartile range (IQR).\n",
    "\n",
    "The IQR method operates using the following equation:\n",
    "\n",
    "$$\n",
    "IQR = Q_3 - Q_1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Q_1$ is the first quartile (25th percentile).\n",
    "- $Q_3$ is the third quartile (75th percentile).\n",
    "\n",
    "Outliers are any data points that fall below the lower bound or above the upper bound:\n",
    "\n",
    "$$\n",
    "\\text{Lower Bound} = Q_1 - 1.5 \\times IQR\n",
    "$$\n",
    "$$\n",
    "\\text{Upper Bound} = Q_3 + 1.5 \\times IQR\n",
    "$$\n",
    "\n",
    "These outliers are removed on a monthly basis to account for seasonal variations in the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_count = len(daily_volumes)\n",
    "trimmed_observed = daily_volumes[daily_volumes['Volume_mcm'] <= maximum_volume]\n",
    "trimmed_count = len(trimmed_observed)\n",
    "removed_samples = original_count - trimmed_count\n",
    "print(f\"Number of samples removed: {removed_samples}\")\n",
    "print(f\"Valid number of samples: {original_count - removed_samples}\")\n",
    "trimmed_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_areas_filtered['date'] = pd.to_datetime(water_areas_filtered['date'], errors='coerce')\n",
    "water_areas_filtered['month'] = water_areas_filtered['date'].dt.month\n",
    "\n",
    "def calculate_min_surface_area(df, full_volume, minimum_volume, maximum_surface_area):\n",
    "    volume_percentage_diff = (maximum_volume - minimum_volume) / maximum_volume    \n",
    "    min_surface_area_estimate =  (maximum_surface_area * (1 - volume_percentage_diff))    \n",
    "    print(f\"Full Supply Volume: {full_volume:.2f} million m³\")\n",
    "    print(f\"Minimum Volume: {minimum_volume:.2f} million m³\")\n",
    "    print(f\"Full Supply Surface Area: {full_surface_area:.2f} ha\")\n",
    "    print(f\"Estimated Minimum Surface Area: {min_surface_area_estimate:.2f} ha\")    \n",
    "    initial_count = len(df)\n",
    "    df_filtered = df[(df['water_area_ha'] >= min_surface_area_estimate)]\n",
    "    removed_count = initial_count - len(df_filtered)    \n",
    "    print(f\"Number of values removed based on min and max thresholds: {removed_count}\")\n",
    "    print(f\"Remaining samples after threshold filtering: {len(df_filtered)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "minimum_volume = trimmed_observed['Volume_mcm'].min()\n",
    "water_areas_filtered = calculate_min_surface_area(water_areas_filtered, maximum_volume, minimum_volume, maximum_surface_area)\n",
    "\n",
    "def remove_outliers_per_month(df):\n",
    "    cleaned_data = df.copy()\n",
    "    total_initial_samples = len(cleaned_data)\n",
    "    \n",
    "    removed_outliers_count = 0\n",
    "    for month, group in df.groupby('month'):\n",
    "        Q1 = group['water_area_ha'].quantile(0.25)\n",
    "        Q3 = group['water_area_ha'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR        \n",
    "        outliers = group[\n",
    "            (group['water_area_ha'] < lower_bound) | \n",
    "            (group['water_area_ha'] > upper_bound)\n",
    "        ]\n",
    "        removed_outliers_count += len(outliers)\n",
    "        cleaned_data.loc[group.index, 'water_area_ha'] = group[\n",
    "            (group['water_area_ha'] >= lower_bound) & \n",
    "            (group['water_area_ha'] <= upper_bound)\n",
    "        ]['water_area_ha']\n",
    "    \n",
    "    remaining_after_outliers = len(cleaned_data.dropna(subset=['water_area_ha']))    \n",
    "    print(f\"Total initial samples: {total_initial_samples}\")\n",
    "    print(f\"Number of samples removed due to outliers: {removed_outliers_count}\")\n",
    "    print(f\"Number of samples remaining after outlier removal: {remaining_after_outliers}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "water_areas_cleaned = remove_outliers_per_month(water_areas_filtered)\n",
    "initial_cleaned_count = len(water_areas_cleaned)\n",
    "water_areas_cleaned.dropna(subset=['water_area_ha'], inplace=True)\n",
    "final_cleaned_count = len(water_areas_cleaned)\n",
    "\n",
    "samples_removed_nan = initial_cleaned_count - final_cleaned_count\n",
    "print(f\"Number of samples removed due to NaN: {samples_removed_nan}\")\n",
    "print(f\"Number of samples remaining after dropping NaN values: {final_cleaned_count}\")\n",
    "water_areas_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preparation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Dam Level Based on Surface Area Using Power Coefficient\n",
    "\n",
    "The **dam level** (or height of water in the dam) can be estimated using a power-law relationship between the surface area and the level. This relationship assumes that as the surface area increases, the dam level increases at a nonlinear rate, which is determined by a power coefficient derived from the reservoir's geometry or rating curve.\n",
    "\n",
    "The formula for calculating the dam level is:\n",
    "\n",
    "$$\n",
    "L = \\left( \\frac{A}{A_{\\text{full}}} \\right)^{\\frac{1}{n}} \\times L_{\\text{full}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $L$ is the **calculated dam level** (height of water) in meters.\n",
    "- $A$ is the current water surface area in hectares.\n",
    "- $A_{\\text{full}}$ is the full surface area of the reservoir in hectares.\n",
    "- $L_{\\text{full}}$ is the dam level at full capacity.\n",
    "- $n$ is the power coefficient, which is derived from the rating curve and represents how the surface area changes with the dam level.\n",
    "\n",
    "In this case, the power coefficient $n$ is dynamically calculated from the reservoir's rating curve data, which reflects the relationship between water surface area and dam level. This dynamic approach allows for a more accurate estimation of the reservoir's dam level based on its current surface area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_law(x, c, n):\n",
    "    return c * np.power(x, n)\n",
    "\n",
    "water_level = rating_curve['water_level'].values\n",
    "volume_mcm = rating_curve['volume_mcm'].values\n",
    "low_volume_threshold = trimmed_observed['Volume_mcm'].quantile(0.25)\n",
    "low_volume_mask = (volume_mcm <= low_volume_threshold)\n",
    "filtered_water_level_low = water_level[low_volume_mask]\n",
    "filtered_volume_mcm_low = volume_mcm[low_volume_mask]\n",
    "\n",
    "if len(filtered_water_level_low) >= 2:\n",
    "    params_low, _ = curve_fit(power_law, filtered_water_level_low, filtered_volume_mcm_low, p0=[1, 1])\n",
    "    c_fitted_low, n_fitted_low = params_low\n",
    "    print(f\"Low Volume Fit - c: {c_fitted_low}, n: {n_fitted_low}\")\n",
    "else:\n",
    "    raise ValueError(\"Not enough data points to fit the low-volume power law.\")\n",
    "\n",
    "mid_high_volume_mask = (volume_mcm > low_volume_threshold)\n",
    "filtered_water_level_mid_high = water_level[mid_high_volume_mask]\n",
    "filtered_volume_mcm_mid_high = volume_mcm[mid_high_volume_mask]\n",
    "\n",
    "if len(filtered_water_level_mid_high) >= 2:\n",
    "    params_mid_high, _ = curve_fit(power_law, filtered_water_level_mid_high, filtered_volume_mcm_mid_high, p0=[1, 1])\n",
    "    c_fitted_mid_high, n_fitted_mid_high = params_mid_high\n",
    "    print(f\"Mid-High Volume Fit - c: {c_fitted_mid_high}, n: {n_fitted_mid_high}\")\n",
    "else:\n",
    "    raise ValueError(\"Not enough data points to fit the mid-to-high-volume power law.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Power Coefficient for Level Calculation\n",
    "\n",
    "Once the power coefficient is calculated dynamically from the rating curve, it can be used to estimate the **dam level** based on the current water surface area. This is done using the inverse power law formula:\n",
    "\n",
    "**Calculate Dam Level from Surface Area**\n",
    "\n",
    "The dam level can be estimated from the surface area using the following relationship:\n",
    "\n",
    "$$\n",
    "L_{\\text{calculated}} = \\left( \\frac{A_{\\text{current}}}{A_{\\text{full}}} \\right)^{\\frac{1}{n}} \\times L_{\\text{full}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $L_{\\text{calculated}}$ is the estimated **dam level**.\n",
    "- $A_{\\text{current}}$ is the current water surface area.\n",
    "- $A_{\\text{full}}$ is the full surface area of the dam at maximum capacity.\n",
    "- $n$ is the power coefficient calculated from the rating curve data.\n",
    "- $L_{\\text{full}}$ is the **full dam level** at maximum capacity.\n",
    "\n",
    "### Explanation:\n",
    "- **`water_area_ha`**: The current surface area in hectares.\n",
    "- **`full_surface_area`**: The surface area of the dam at 100% capacity (maximum water surface area), which is extracted from the dam attributes.\n",
    "- **`full_dam_level`**: The height of the water body at maximum capacity (when the dam is full).\n",
    "- **`n_fitted`**: The power coefficient that describes how the surface area changes with dam level. This coefficient is derived from the rating curve using a power-law relationship.\n",
    "\n",
    "This coefficient is derived from the power-law relationship between the water level at the dam wall and the surface area of the reservoir. It is used to estimate changes in dam level as a function of surface area. By leveraging this coefficient, we can estimate the water level based on the surface area, assuming that the surface area scales predictably with the dam level.\n",
    "\n",
    "This relationship is particularly useful when direct measurements of water level are unavailable, but surface area data is available (e.g., from satellite imagery).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert conditions\n",
    "assert maximum_surface_area > 0, \"maximum_surface_area must be greater than 0.\"\n",
    "assert maximum_dam_level > 0, \"maximum_dam_level must be greater than 0.\"\n",
    "assert 'c_fitted_low' in globals() and 'n_fitted_low' in globals(), \"Low-volume power-law parameters must be defined.\"\n",
    "assert 'c_fitted_mid_high' in globals() and 'n_fitted_mid_high' in globals(), \"Mid-to-high-volume power-law parameters must be defined.\"\n",
    "\n",
    "# Clean up the water_areas_cleaned DataFrame\n",
    "if 'date' in water_areas_cleaned.columns:\n",
    "    water_areas_cleaned = water_areas_cleaned.drop(columns=['date']).copy(deep=True)  # Make a deep copy\n",
    "\n",
    "if 'date' not in water_areas_cleaned.columns:\n",
    "    water_areas_cleaned = water_areas_cleaned.reset_index(drop=False).copy(deep=True)  # Make a deep copy after reset_index\n",
    "\n",
    "if 'date' not in water_areas_cleaned.columns:\n",
    "    print(f\"'date' column not found. Current columns: {water_areas_cleaned.columns}\")\n",
    "\n",
    "# Make a distinct deep copy of the DataFrame to avoid any unwanted views\n",
    "water_areas_cleaned = water_areas_cleaned.copy(deep=True)\n",
    "\n",
    "# Function to calculate dam level from surface area\n",
    "def calculate_level_from_area(area_ha, volume_threshold, c_low, n_low, c_mid_high, n_mid_high):\n",
    "    if area_ha <= volume_threshold:\n",
    "        return (area_ha / maximum_surface_area) ** (1 / n_low) * maximum_dam_level\n",
    "    else:\n",
    "        return (area_ha / maximum_surface_area) ** (1 / n_mid_high) * maximum_dam_level\n",
    "\n",
    "# Apply the calculation function to the water_area_ha column\n",
    "water_areas_cleaned['calculated_level'] = water_areas_cleaned['water_area_ha'].apply(\n",
    "    lambda area: calculate_level_from_area(area, low_volume_threshold, c_fitted_low, n_fitted_low, c_fitted_mid_high, n_fitted_mid_high)\n",
    ")\n",
    "\n",
    "# Make an explicit deep copy of trimmed_observed to avoid any slices\n",
    "trimmed_observed = trimmed_observed.copy(deep=True)\n",
    "# Convert 'Date' column in trimmed_observed DataFrame to datetime\n",
    "trimmed_observed.loc[:, 'Date'] = pd.to_datetime(trimmed_observed['Date'])\n",
    "\n",
    "# Ensure 'date' is in datetime format in water_areas_cleaned\n",
    "if 'date' in water_areas_cleaned.columns:\n",
    "    # Use .loc to modify 'date' safely, ensuring no warning\n",
    "    water_areas_cleaned.loc[:, 'date'] = pd.to_datetime(water_areas_cleaned['date'])\n",
    "else:\n",
    "    raise KeyError(\"'date' column not found in water_areas_cleaned\")\n",
    "\n",
    "# Merge the two DataFrames on their date columns\n",
    "merged_data = pd.merge(trimmed_observed, water_areas_cleaned, left_on='Date', right_on='date', how='inner')\n",
    "merged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Dam Level\n",
    "ax.plot(merged_data['Date'], merged_data['Dam_Level'], \n",
    "        label='Dam Level', color='blue', linewidth=1)\n",
    "\n",
    "# Plot Calculated Level\n",
    "ax.plot(merged_data['Date'], merged_data['calculated_level'], \n",
    "        label='Calculated Level', color='red', linewidth=1)\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title('Dam Level vs Calculated Level Over Time')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Level (m)')\n",
    "plt.xticks(rotation=-45)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.legend(title='Legend', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "# image_path = \"dam_levels_chart_matplotlib.png\"\n",
    "# plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Features and Target for Model Training\n",
    "\n",
    "Since we are trying to predict the **dam level** (`Dam_Level`), the target variable needs to reflect the observed dam level.\n",
    "\n",
    "- **Features**: The input variables used to predict the dam level. In this case, the features include:\n",
    "  - `calculated_level`: The estimated dam level, calculated using the power-law relationship between volume and dam level.\n",
    "  - `water_area_ha`: The current water surface area in hectares.\n",
    "\n",
    "- **Target**: The output variable the model will try to predict, which in this case is the **dam level** (`Dam_Level`).\n",
    "\n",
    "This is implemented with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing features and target for model training...\")\n",
    "merged_data['full_volume'] = full_volume\n",
    "merged_data['full_surface_area'] = full_surface_area\n",
    "merged_data['full_dam_level'] = full_dam_level\n",
    "merged_data['maximum_dam_level'] = maximum_dam_level\n",
    "merged_data['capacity_of_spillway'] = capacity_of_spillway\n",
    "merged_data['vertical_drop'] = vertical_drop\n",
    "merged_data['mean_depth'] = mean_depth\n",
    "merged_data['full_capacity_elevation'] = full_capacity_elevation\n",
    "merged_data['shoreline_length'] = shoreline_length\n",
    "merged_data['maximum_volume'] = maximum_volume\n",
    "merged_data['maximum_surface_area'] = maximum_surface_area\n",
    "\n",
    "\n",
    "print(\"saving preprocesed model data...\")\n",
    "merged_data.to_csv(\"data/preprocess_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Additional information\n",
    "\n",
    "<b> License </b> The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
    "\n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "<b> Contact </b> If you need assistance, please post a question on the [DE Africa Slack channel](https://digitalearthafrica.slack.com/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "<b> Compatible datacube version </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- Garcia Andarcia, M., Dickens, C., Silva, P., Matheswaran, K., & Koo, J. (2024). Digital Twin for management of water resources in the Limpopo River Basin: a concept. Colombo, Sri Lanka: International Water Management Institute (IWMI). CGIAR Initiative on Digital Innovation. 4p.\n",
    "- Gurusinghe, T., Muthuwatta, L., Matheswaran, K., & Dickens, C. (2024). Developing a foundational hydrological model for the Limpopo River Basin using the Soil and Water Assessment Tool Plus (SWAT+). Colombo, Sri Lanka: International Water Management Institute (IWMI). CGIAR Initiative on Digital Innovation. 14p.\n",
    "- Leitão, P. C., Santos, F., Barreiros, D., Santos, H., Silva, P., Madushanka, T., Matheswaran, K., Mutuwatte, L., Vickneswaran, K., Retief, H., Dickens, C., Garcia Andarcia, M. (2024). Operational SWAT+ Model: Advancing Seasonal Forecasting in the Limpopo River Basin. Colombo, Sri Lanka:  International Water Management Institute (IWMI). CGIAR Initiative on Digital Innovation.\n",
    "- Mallick, Archita & Ghosh, Surajit & De Sarkar, Kounik & Roy, Sudip. (2024). Reservoir Water Level Forecasting using Deep Learning Technique. Conference: 2024 IEEE India Geoscience and Remote Sensing Symposium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project background: \n",
    "The CGIAR Digital Innovation Initiative accelerates the transformation towards sustainable and inclusive agrifood systems by generating research-based evidence and innovative digital solutions. It is one of 32 initiatives of CGIAR, a global research partnership for a food-secure future, dedicated to transforming food, land, and water systems in a climate crisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributors\n",
    "\n",
    "**Hugo Retief**  \n",
    "*Researcher*  \n",
    "Email: [hugo@award.org.za](mailto:hugo@award.org.za)  \n",
    "\n",
    "**Surajith Ghosh**  \n",
    "*Researcher*  \n",
    "Email: [S.Ghosh@cgiar.org](mailto:S.Ghosh@cgiar.org)  \n",
    "\n",
    "**Victoria Neema**  \n",
    "*Earth Observation Scientist*  \n",
    "Email: [victoria.neema@digitalearthafrica.org](mailto:victoria.neema@digitalearthafrica.org)  \n",
    "\n",
    "**Kayathri Vigneswaran**  \n",
    "*Junior Data Scientist*  \n",
    "Email: [v.kayathri@cgiar.org](mailto:v.kayathri@cgiar.org)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last Tested:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
