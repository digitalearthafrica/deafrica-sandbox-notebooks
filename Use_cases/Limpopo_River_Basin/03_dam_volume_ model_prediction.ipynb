{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling dam volumes using DEA waterbodies\n",
    "\n",
    "**Products used:** \n",
    "[DE Africa Waterbodies](https://docs.digitalearthafrica.org/en/latest/data_specs/Waterbodies_specs.html), \n",
    "[Department of Water Affairs and Sanitation, South Africa Dam Level and Volume Data](https://www.dws.gov.za/Hydrology/Verified/hymain.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : *Model Prediction and Error Evaluation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Digital Twin (DT)\n",
    "The CGIAR Digital Twin initiative creates dynamic virtual models that combine real-time data, AI, and simulations to improve decision-making. Its prototype for the Limpopo River Basin focuses on enhancing water resource management and conservation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Description\n",
    "This notebook presents a workflow for predicting dam levels and volumes using water surface area data from DE Africa's Waterbodies product, integrating data preprocessing, feature extraction, and Gradient Boosting modeling. \n",
    "\n",
    "As part of the CGIAR Initiative on Digital Innovation, this work contributes to a prototype Digital Twin for the Limpopo River Basin, designed to support real-time decision-making in water management. The Digital Twin leverages AI-driven tools to visualize and simulate the impact of decisions on the basin's ecosystem. To enhance prediction reliability, the model includes a correction mechanism to address unrealistic large drops in dam volume estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import datacube\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from deafrica_tools.waterbodies import get_waterbody, get_time_series, display_time_series\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading data\n",
    "\n",
    "This dataset contains raw water levels data collected from DEA (Department of Environmental Affairs) in South Africa. The cell below reads this ancillary data necessary to conduct the volume prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_curve = pd.read_csv('data/DWS/rating_curve.csv')\n",
    "daily_volumes = pd.read_csv('data/DWS/daily_volumes.csv')\n",
    "dam_attributes = pd.read_csv('data/dam_attributes/dam_attributes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date columns to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_volumes = daily_volumes.copy()\n",
    "daily_volumes['Date'] = pd.to_datetime(daily_volumes['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_volume = dam_attributes[dam_attributes['parameter'] == 'full_volume']['value'].values[0]\n",
    "full_surface_area = dam_attributes[dam_attributes['parameter'] == 'full_surface_area']['value'].values[0]\n",
    "full_dam_level = dam_attributes[dam_attributes['parameter'] == 'full_dam_level']['value'].values[0]\n",
    "maximum_dam_level = dam_attributes[dam_attributes['parameter'] == 'maximum_dam_level']['value'].values[0]\n",
    "capacity_of_spillway = dam_attributes[dam_attributes['parameter'] == 'capacity_of_spillway']['value'].values[0]\n",
    "vertical_drop = dam_attributes[dam_attributes['parameter'] == 'vertical_drop']['value'].values[0]\n",
    "mean_depth = dam_attributes[dam_attributes['parameter'] == 'mean_depth']['value'].values[0]\n",
    "full_capacity_elevation = dam_attributes[dam_attributes['parameter'] == 'full_capacity_elevation']['value'].values[0]\n",
    "shoreline_length = dam_attributes[dam_attributes['parameter'] == 'shoreline_length']['value'].values[0]\n",
    "maximum_volume = dam_attributes[dam_attributes['parameter'] == 'maximum_volume']['value'].values[0]\n",
    "maximum_surface_area = dam_attributes[dam_attributes['parameter'] == 'maximum_surface_area']['value'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_count = len(daily_volumes)\n",
    "trimmed_observed = daily_volumes[daily_volumes['Volume_mcm'] <= maximum_volume]\n",
    "trimmed_count = len(trimmed_observed)\n",
    "removed_samples = original_count - trimmed_count\n",
    "print(f\"Number of samples removed: {removed_samples}\")\n",
    "print(f\"Valid number of samples: {original_count - removed_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Dam Levels and Convert to Volumes Using the Rating Curve\n",
    "\n",
    "After training the Gradient Boosting Regressor model, the next step is to predict the **dam levels** for the entire dataset and convert both the predicted and observed dam levels into corresponding **water volumes** using the **rating curve**.\n",
    "\n",
    "Once the model is trained, we use it to predict the dam levels based on the input features (`calculated_level` and `water_area_ha`). The predicted dam levels are stored in `y_pred_full`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imputed_df = pd.read_csv(\"data/prediction_data.csv\")\n",
    "target = pd.read_csv(\"data/test_data.csv\")\n",
    "merged_data = pd.read_csv(\"data/preprocess_data.csv\")\n",
    "merged_data['Date']= pd.to_datetime(merged_data['Date'])\n",
    "\n",
    "target = pd.DataFrame(target, columns=['Dam_Level'])\n",
    "target = target['Dam_Level']\n",
    "y_pred_full = features_imputed_df['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot observed vs predicted levels over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframes\n",
    "observed_levels_df = pd.DataFrame({\n",
    "    'Date': merged_data['Date'],\n",
    "    'Observed Level': merged_data['Dam_Level']\n",
    "})\n",
    "\n",
    "predicted_levels_df = pd.DataFrame({\n",
    "    'Date': merged_data['Date'],\n",
    "    'Predicted Level': y_pred_full\n",
    "})\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Observed Level\n",
    "ax.plot(observed_levels_df['Date'], observed_levels_df['Observed Level'], \n",
    "        label='Observed Level', color='blue', linewidth=1)\n",
    "\n",
    "# Plot Predicted Level (using scatter for markers)\n",
    "ax.scatter(predicted_levels_df['Date'], predicted_levels_df['Predicted Level'], \n",
    "           label='Predicted Level', color='red', marker='x', s=30)\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title('Observed vs Predicted Dam Levels Over Time')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Dam Level (m)')\n",
    "plt.xticks(rotation=-45)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.legend(title='Legend', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "# image_path = \"observed_vs_predicted_dam_levels_matplotlib.png\"\n",
    "# plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the predicted levels vs. observed levels against entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_full = mean_squared_error(target, y_pred_full)\n",
    "rmse_full = np.sqrt(mse_full)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_full = mean_absolute_percentage_error(target, y_pred_full)\n",
    "r2_full = r2_score(target, y_pred_full)\n",
    "print(f\"{'RMSE':<10} {'MAPE':<10} {'R² Score':<10}\")\n",
    "print(f\"{rmse_full:<10.4f} {mape_full:<10.2f} {r2_full:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dam level volume interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the rating curve to map the **dam levels** to **volumes**. The rating curve provides a relationship between **dam level** and **volume** for the reservoir, and we use interpolation to make this mapping. The `interpolate.interp1d` function is used to create a mapping from dam levels to volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_curve_interp_volume = interpolate.interp1d(rating_curve['water_level'], rating_curve['volume_mcm'], fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpolation allows us to take both the observed dam levels (target.values) and the predicted dam levels (y_pred_full) and convert them into volumes using the rating curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_volumes_full = rating_curve_interp_volume(target.values)\n",
    "predicted_volumes_full = rating_curve_interp_volume(y_pred_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Maximum Percentage Drop Between Consecutive Actual Volume Readings\n",
    "\n",
    "##### Why We Do This:\n",
    "- **Identify Realistic Drops**: By calculating the maximum percentage drop between consecutive readings, we can identify what levels of volume depletion are realistically expected. This helps distinguish between normal variations and extreme cases.\n",
    "  \n",
    "- **Model Correction**: If the model drastically under-predicts volumes (for example, when predicted volumes show unrealistically large drops), this analysis allows us to apply corrections. By identifying the normal range of percentage drops, we can use this information to adjust the model predictions and prevent unrealistic under-predictions.\n",
    "\n",
    "- **Improve Prediction Accuracy**: Understanding typical percentage drops between readings enables us to refine the model. If predicted drops fall outside of expected ranges, corrective actions can be taken, ensuring that model outputs better match observed patterns.\n",
    "\n",
    "- The percentage drop between consecutive actual readings is calculated by comparing the current volume with the previous volume. The formula used is:\n",
    "\n",
    "$$\n",
    "\\text{percentDrop} = \\frac{(\\text{previous}_{volume} - \\text{current}_{volume})}{\\text{previous}_{volume}} \\times 100\n",
    "$$\n",
    "\n",
    "\n",
    "- This calculates how much the volume has decreased as a percentage of the previous volume\n",
    "\n",
    "This step is crucial to maintain the reliability of model outputs and ensure that predicted water volumes remain realistic based on historical trends and known behavior of water depletion in the dam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_volumes = daily_volumes.sort_values(by='Date')\n",
    "daily_volumes['previous_volume'] = daily_volumes['Volume_mcm'].shift(1)\n",
    "daily_volumes['percent_drop'] = (daily_volumes['previous_volume'] - daily_volumes['Volume_mcm']) / daily_volumes['previous_volume'] * 100\n",
    "max_percent_drop = daily_volumes['percent_drop'].max()\n",
    "print(f\"Max Percent Drop between actual readings: {max_percent_drop}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Predicted Data and Applying the Maximum Percent Drop Rule\n",
    "\n",
    "In this section, we first create the `predicted_data` DataFrame, which stores the predicted dam volumes along with their corresponding dates. The predicted volumes are derived from earlier model outputs based on observed water levels. \n",
    "\n",
    "The main purpose of this step is to apply a validation rule known as the **maximum percent drop rule**. This rule ensures that the predicted volumes do not show unrealistic, sharp declines between consecutive data points, especially when there are large time gaps between observations. By comparing the predicted dam volume at each time step with the previous one, the rule checks if the drop in volume exceeds the maximum percentage drop observed historically. If the actual drop exceeds this threshold, the predicted volume is adjusted to fall within reasonable limits.\n",
    "\n",
    "To account for varying time gaps between readings, the **median time difference** between consecutive observations is calculated and used to apply the rule only when the time difference between consecutive readings is smaller than or equal to this median. This helps ensure that the predicted dam volumes are consistent with the natural behavior of water level changes over time, particularly in cases where data points are sparse and sporadic.\n",
    "\n",
    "Finally, the rule is applied to the `predicted_data`, correcting any instances where the predicted drop in volume is considered unrealistic, maintaining the integrity and accuracy of the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = pd.DataFrame({\n",
    "    'date': merged_data['Date'],\n",
    "    'Predicted_Dam_Volume': predicted_volumes_full\n",
    "})\n",
    "\n",
    "def apply_max_percent_drop_rule(data, max_percent_drop, median_time_diff):\n",
    "    data = data.sort_values(by='date')\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "        prev_value = data.iloc[i-1]['Predicted_Dam_Volume']\n",
    "        curr_value = data.iloc[i]['Predicted_Dam_Volume']\n",
    "        time_diff = (data.iloc[i]['date'] - data.iloc[i-1]['date']).days\n",
    "        \n",
    "        if time_diff <= median_time_diff and curr_value < prev_value:\n",
    "            max_allowed_drop = prev_value * (max_percent_drop / 100.0)\n",
    "            if (prev_value - curr_value) > max_allowed_drop:\n",
    "                # Adjust the predicted volume to enforce the rule\n",
    "                data.at[data.index[i], 'Predicted_Dam_Volume'] = prev_value - max_allowed_drop\n",
    "    \n",
    "    return data\n",
    "\n",
    "daily_volumes['time_diff_days'] = daily_volumes['Date'].diff().dt.days\n",
    "median_time_diff = daily_volumes['time_diff_days'].median()\n",
    "predicted_data = apply_max_percent_drop_rule(predicted_data, max_percent_drop, median_time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the predicted volumes vs. observed volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_volumes = merged_data['Volume_mcm'].values\n",
    "predicted_volumes = predicted_data['Predicted_Dam_Volume'].values\n",
    "\n",
    "min_length = min(len(observed_volumes), len(predicted_volumes))\n",
    "observed_volumes = observed_volumes[:min_length]\n",
    "predicted_volumes = predicted_volumes[:min_length]\n",
    "\n",
    "mse_volumes = mean_squared_error(observed_volumes, predicted_volumes)\n",
    "\n",
    "rmse_volumes = np.sqrt(mse_volumes)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_volumes = mean_absolute_percentage_error(observed_volumes, predicted_volumes)\n",
    "\n",
    "r2_volumes = r2_score(observed_volumes, predicted_volumes)\n",
    "\n",
    "# Output results\n",
    "print(f\"{'Metric':<10} {'RMSE':<10} {'MAPE':<10} {'R² Score':<10}\")\n",
    "print(f\"{'Volumes':<10} {rmse_volumes:<10.4f} {mape_volumes:<10.2f} {r2_volumes:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot observed vs predicted volumes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframes\n",
    "observed_volumes_full_df = pd.DataFrame({\n",
    "    'Date': trimmed_observed['Date'],\n",
    "    'Observed Volume': trimmed_observed['Volume_mcm']\n",
    "})\n",
    "\n",
    "predicted_volumes_full_df = pd.DataFrame({\n",
    "    'Date': merged_data['Date'],\n",
    "    'Predicted Volume': predicted_volumes_full\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Observed Volume\n",
    "ax.plot(observed_volumes_full_df['Date'], observed_volumes_full_df['Observed Volume'], \n",
    "        label='Observed Volume', color='blue', linewidth=1)\n",
    "\n",
    "# Plot Predicted Volume (using scatter for markers)\n",
    "ax.scatter(predicted_volumes_full_df['Date'], predicted_volumes_full_df['Predicted Volume'], \n",
    "           label='Predicted Volume', color='red', marker='x', s=30)\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title('Observed vs Predicted Volumes Over Time')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Volume (mcm)')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "# Move legend outside the plot\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.legend(title='Legend', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "# image_path = \"observed_vs_predicted_volumes_matplotlib.png\"\n",
    "# plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pd.merge(observed_volumes_full_df, predicted_volumes_full_df, on='Date', how='left')\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "output_data.to_csv('results/volume_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- Garcia Andarcia, M., Dickens, C., Silva, P., Matheswaran, K., & Koo, J. (2024). Digital Twin for management of water resources in the Limpopo River Basin: a concept. Colombo, Sri Lanka: International Water Management Institute (IWMI). CGIAR Initiative on Digital Innovation. 4p.\n",
    "- Gurusinghe, T., Muthuwatta, L., Matheswaran, K., & Dickens, C. (2024). Developing a foundational hydrological model for the Limpopo River Basin using the Soil and Water Assessment Tool Plus (SWAT+). Colombo, Sri Lanka: International Water Management Institute (IWMI). CGIAR Initiative on Digital Innovation. 14p.\n",
    "- Leitão, P. C., Santos, F., Barreiros, D., Santos, H., Silva, P., Madushanka, T., Matheswaran, K., Mutuwatte, L., Vickneswaran, K., Retief, H., Dickens, C., Garcia Andarcia, M. (2024). Operational SWAT+ Model: Advancing Seasonal Forecasting in the Limpopo River Basin. Colombo, Sri Lanka:  International Water Management Institute (IWMI). CGIAR Initiative on Digital Innovation.\n",
    "- Mallick, Archita & Ghosh, Surajit & De Sarkar, Kounik & Roy, Sudip. (2024). Reservoir Water Level Forecasting using Deep Learning Technique. Conference: 2024 IEEE India Geoscience and Remote Sensing Symposium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Additional information\n",
    "\n",
    "<b> License </b> The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
    "\n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "<b> Contact </b> If you need assistance, please post a question on the [DE Africa Slack channel](https://digitalearthafrica.slack.com/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "<b> Compatible datacube version </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed into project background: \n",
    "The CGIAR Digital Innovation Initiative accelerates the transformation towards sustainable and inclusive agrifood systems by generating research-based evidence and innovative digital solutions. It is one of 32 initiatives of CGIAR, a global research partnership for a food-secure future, dedicated to transforming food, land, and water systems in a climate crisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributors\n",
    "\n",
    "**Hugo Retief**  \n",
    "*Researcher*  \n",
    "Email: [hugo@award.org.za](mailto:hugo@award.org.za)  \n",
    "\n",
    "**Victoria Neema**  \n",
    "*Earth Observation Scientist*  \n",
    "Email: [victoria.neema@digitalearthafrica.org](mailto:victoria.neema@digitalearthafrica.org)  \n",
    "\n",
    "**Kayathri Vigneswaran**  \n",
    "*Junior Data Scientist*  \n",
    "Email: [v.kayathri@cgiar.org](mailto:v.kayathri@cgiar.org)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last Tested:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
