{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate, optimize, and fit a classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Before we can begin making crop/non-crop predictions, we first need to evaluate, optimize, and fit a classifier.  Because we are working with spatial data, and because the size of our training dataset is relatively small (by machine learning standards), we need to implement some lesser known methods for evaluating and optimizing our model.  These include implementing spatially explicit k-fold cross-validation techniques, running nested k-fold cross validation, and fitting a model on our entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this notebook, we will use the training data collected in the first notebook (`1_Extract_training_data.ipynb`) to fit and evaluate a classifier.  \n",
    "\n",
    "The steps undertaken are:\n",
    "1. Spatially cluster our training data to visualize spatial groupings\n",
    "2. Calculate an unbiased performance estimate via **nested, spatial K-fold cross-validation**.  \n",
    "3. Optimize the hyperparameters of the model using `GridSearchCV`\n",
    "4. Fit a model on _all_ the data using the parameters identified in step 3\n",
    "5. Save the model to disk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, KFold\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, balanced_accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `Classifier`: This parameter refers to the scikit-learn classification model to use, first uncomment the classifier of interest in the `Load Packages` section and then enter the function name into this parameter `e.g. Classifier = RandomForestClassifier`   \n",
    "* `metric` : A single string that denotes the scorer used to optimize the model. See the scoring parameter page [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for a pre-defined list of options. For binary classifications, 'F1' or 'balanced_accuracy' are good metrics.\n",
    "* `output_suffix`: A suffix to add to the exported model corresponding to the model iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"results/southern_training_data_20220225_2021.txt\"\n",
    "\n",
    "Classifier = RandomForestClassifier\n",
    "\n",
    "metric = 'balanced_accuracy' \n",
    "\n",
    "output_suffix = '20220225_2021'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Parameters\n",
    "\n",
    "* `inner_cv_splits` : Number of cross-validation splits to do on the inner loop, e.g. `5`\n",
    "* `outer_cv_splits` : Number of cross-validation splits to do on the outer loop, e.g. `5`\n",
    "* `test_size` : This will determine what fraction of the dataset will be set aside as the testing dataset. There is a trade-off here between having a larger test set that will help us better determine the quality of our classifier, and leaving enough data to train the classifier. A good deafult is to set 10-20 % of your dataset aside for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv_splits = 5\n",
    "\n",
    "outer_cv_splits = 7\n",
    "\n",
    "test_size = 0.20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically find the number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 15\n"
     ]
    }
   ],
   "source": [
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training  and coordinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "# coordinates = np.loadtxt(coordinate_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:]\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]\n",
    "\n",
    "#convert variable names into sci-kit learn nomenclature\n",
    "X = model_input[:, model_col_indices]\n",
    "y = model_input[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the classifier\n",
    "\n",
    "Now that we're happy with the spatial clustering, we can evaluate the classifier via _nested_, K-fold cross-validation.\n",
    "\n",
    "The k-fold cross-validation procedure is used to estimate the performance of machine learning models when making predictions on data not used during training. However, when the same cross-validation procedure and dataset are used to both tune the hyperparameters and select a model, it is likely to lead to an optimistically biased evaluation of the model performance.\n",
    "\n",
    "One approach to overcoming this bias is to nest the hyperparameter optimization procedure under the model selection procedure. This is called nested cross-validation and is the preferred way to evaluate and compare tuned machine learning models.\n",
    "            \n",
    "\n",
    "***\n",
    "\n",
    "Before evaluating the model, we need to set up some hyperparameters to test during optimization.  The `param_grid` object below is set up to test various important hyperparameters for a Random Forest model. \n",
    "\n",
    "> **Note**: the parameters in the `param_grid` object depend on the classifier being used. This notebook is set up for a random forest classifier, to adjust the paramaters to suit a different classifier, look up the important parameters under the relevant [sklearn documentation](https://scikit-learn.org/stable/supervised_learning.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'max_features': ['auto', 'log2', None],\n",
    "    'n_estimators': [200,250,300,350,400],\n",
    "    'criterion':['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will conduct the nested CV using the SKCV function. This will take a while to run depending on the number of inner and outer cv splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 7/7 outer cv split\r"
     ]
    }
   ],
   "source": [
    "outer_cv = KFold(n_splits=outer_cv_splits, shuffle=True,\n",
    "                        random_state=0)\n",
    "\n",
    "# lists to store results of CV testing\n",
    "acc = []\n",
    "f1 = []\n",
    "roc_auc = []\n",
    "i = 1\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    print(f\"Working on {i}/{outer_cv_splits} outer cv split\", end='\\r')\n",
    "    model = Classifier(random_state=1)\n",
    "\n",
    "    # index training, testing, and coordinate data\n",
    "    X_tr, X_tt = X[train_index, :], X[test_index, :]\n",
    "    y_tr, y_tt = y[train_index], y[test_index]\n",
    "    \n",
    "    # inner split on data within outer split\n",
    "    inner_cv = KFold(n_splits=inner_cv_splits,\n",
    "                     shuffle=True,\n",
    "                     random_state=0)\n",
    "    \n",
    "    clf = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=metric,\n",
    "        n_jobs=ncpus,\n",
    "        refit=True,\n",
    "        cv=inner_cv.split(X_tr, y_tr),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    # predict using the best model\n",
    "    best_model = clf.best_estimator_\n",
    "    pred = best_model.predict(X_tt)\n",
    "\n",
    "    # evaluate model w/ multiple metrics\n",
    "    # ROC AUC\n",
    "    probs = best_model.predict_proba(X_tt)\n",
    "    probs = probs[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_tt, probs)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    roc_auc.append(auc_)\n",
    "    # Overall accuracy\n",
    "    ac = balanced_accuracy_score(y_tt, pred)\n",
    "    acc.append(ac)\n",
    "    # F1 scores\n",
    "    f1_ = f1_score(y_tt, pred)\n",
    "    f1.append(f1_)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results of our model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Nested K-Fold Cross-Validation Scores ===\n",
      "Mean balanced accuracy: 0.82\n",
      "Std balanced accuracy: 0.02\n",
      "\n",
      "\n",
      "Mean F1: 0.78\n",
      "Std F1: 0.02\n",
      "\n",
      "\n",
      "Mean roc_auc: 0.912\n",
      "Std roc_auc: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Nested K-Fold Cross-Validation Scores ===\")\n",
    "print(\"Mean balanced accuracy: \"+ str(round(np.mean(acc), 2)))\n",
    "print(\"Std balanced accuracy: \"+ str(round(np.std(acc), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean F1: \"+ str(round(np.mean(f1), 2)))\n",
    "print(\"Std F1: \"+ str(round(np.std(f1), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean roc_auc: \"+ str(round(np.mean(roc_auc), 3)))\n",
    "print(\"Std roc_auc: \"+ str(round(np.std(roc_auc), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores represent a robust estimate of the accuracy of our classifier. However, because we are using only a subset of data to fit and optimize the models, it is reasonable to expect these scores are an under-estimate of the final model's accuracy.  Also, the _map_ accuracy will differ from the accuracies reported here since the training data is not a perfect representation of the data in the real world (e.g. if we have purposively over-sampled from hard-to-classify regions, or if the proportions of crop to non-crop do not match the proportions in the real world). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters\n",
    "\n",
    "Hyperparameter searches are a required process in machine learning. Machine learning models require certain “hyperparameters”; model parameters that can be learned from the data. Finding good values for these parameters is a “hyperparameter search” or an “hyperparameter optimization.”\n",
    "\n",
    "To optimize the parameters in our model, we use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to exhaustively search through a set of parameters and determine the combination that will result in the highest accuracy based upon the accuracy metric defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n_splits of train-test_split\n",
    "rs = ShuffleSplit(n_splits=outer_cv_splits, test_size=test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 30 candidates, totalling 210 fits\n",
      "The most accurate combination of tested parameters is: \n",
      "{'criterion': 'entropy', 'max_features': None, 'n_estimators': 300}\n",
      "\n",
      "\n",
      "The balanced_accuracy score using these parameters is: \n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "#instatiate a gridsearchCV\n",
    "clf = GridSearchCV(Classifier(),\n",
    "                   param_grid,\n",
    "                   scoring=metric,\n",
    "                   verbose=1,\n",
    "                   cv=rs.split(X, y),\n",
    "                   n_jobs=ncpus)\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print('\\n')\n",
    "print(\"The \"+metric+\" score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model\n",
    "\n",
    "Using the best parameters from our hyperparmeter optmization search, we now fit our model on all the data to give the best possible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_features=None, n_estimators=300,\n",
       "                       n_jobs=15, random_state=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new model\n",
    "new_model = Classifier(**clf.best_params_, random_state=1, n_jobs=ncpus)\n",
    "new_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "Running this cell will export the classifier as a binary`.joblib` file. This will allow for importing the model in the subsequent script, `4_Predict.ipynb` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'results/southern_ml_model_'+output_suffix+'.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/southern_ml_model_20220225_2021.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(new_model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To continue working through the notebooks in this `Southern Africa Cropland Mask` workflow, go to the next notebook `4_Predict.ipynb`.\n",
    "\n",
    "1. [Extract_training_data](1_Extract_training_data.ipynb) \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. **Train_fit_evaluate_classifier (this notebook)**\n",
    "4. [Predict](4_Predict.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
